{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitcondad36d02735c7842b49397084c546dac98",
   "display_name": "Python 3.7.4 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_selection\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "npf_train = pd.read_csv(\"data/npf_train.csv\")\n",
    "npf_test = pd.read_csv(\"data/npf_test_hidden.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprosessing(npf,scaler):\n",
    "    '''Preprosessing function for npf_*.csv files'''\n",
    "\n",
    "    # Dropping features 'partlybad','id' and 'date' because we won't need them. Feature 'partlybad' was only False \n",
    "    X = npf.drop(['date','id','partlybad','class4'],axis=1)\n",
    "    \n",
    "\n",
    "    #Dropping std's\n",
    "    X_means = X.drop([c for c in npf.columns if 'std' in c],axis=1)\n",
    "\n",
    "\n",
    "    # Removing .means from all column names\n",
    "    cols = [col[:-5] for col in X_means.columns]\n",
    "\n",
    "    # Normalizing for zero mean and unit variance\n",
    "    X_means_np = scaler(X_means)\n",
    "\n",
    "    #df = pd.DataFrame(X_means_np, columns=cols)\n",
    "    #df[\"class4\"] = npf[\"class4\"]\n",
    "\n",
    "    #return df\n",
    "    X_means['class4'] = npf['class4']\n",
    "    return X_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_columns(X_train, y_train, n):\n",
    "    '''Return n best feature columns'''\n",
    "    select = sklearn.feature_selection.SelectKBest(k=n)\n",
    "    selected_features = select.fit(X_train, y_train)\n",
    "    indices_selected = selected_features.get_support(indices=True)\n",
    "    colnames_selected = [X_train.columns[i] for i in indices_selected]\n",
    "\n",
    "    return colnames_selected"
   ]
  },
  {
   "source": [
    "Loading the data and doing some preprosessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "npf = preprosessing(npf_train,scale)\n",
    "X_npf = npf.drop('class4',axis=1)\n",
    "#y = npf['class4']\n",
    "\n",
    "# Changing the categorical values to integers\n",
    "y = npf['class4'].astype('category').cat.codes"
   ]
  },
  {
   "source": [
    "Scaling the data for zero mean and unit variance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Removing .means from all column names\n",
    "    cols = [col[:-5] for col in X_npf.columns]\n",
    "\n",
    "    # Normalizing for zero mean and unit variance\n",
    "    X_np = scale(X_npf)\n",
    "\n",
    "    X = pd.DataFrame(X_np, columns=cols)"
   ]
  },
  {
   "source": [
    "Selecting 20 best features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = best_feature_columns(X,y,20)\n",
    "#features = X.columns"
   ]
  },
  {
   "source": [
    "Now doing the clustering. Fist using K-Means clustering. We set 4 clusters as parameter because we know that we have 4 classes to distinguish"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=4, n_init=50, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=3, tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "kmeans_cluster = KMeans(4,n_init=50,random_state=3)\n",
    "kmeans_cluster.fit(X)\n",
    "#kmeans_cluster = KMeans(n_clusters=4, n_init=50,random_state=42).fit(X[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.16279069767441862"
      ]
     },
     "metadata": {},
     "execution_count": 353
    }
   ],
   "source": [
    "acc = accuracy_score(y, kmeans_cluster.labels_)\n",
    "acc"
   ]
  },
  {
   "source": [
    "Hmm quite bad accuracy. The labels are probably permuted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation = []\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]  # Choose the most common label among data points in the cluster\n",
    "        permutation.append(new_label)\n",
    "    return permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "permutation = find_permutation(4,y,kmeans_cluster.labels_)\n",
    "print(permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys([3, 2, 0, 1])\ndict_values([215, 83, 106, 26])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y).keys())\n",
    "print(Counter(y).values())\n"
   ]
  },
  {
   "source": [
    "Hmm why it misses value 1?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is 0.6069767441860465\n"
     ]
    }
   ],
   "source": [
    "new_labels = [ permutation[label] for label in kmeans_cluster.labels_]   # permute the labels\n",
    "print(\"Accuracy score is\", accuracy_score(y, new_labels))\n"
   ]
  },
  {
   "source": [
    "Let's do a confusion matrix to double check the results\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Clusters   0    1   2  3\n",
       "Labels                  \n",
       "0         35   13  58  0\n",
       "1          3    7  16  0\n",
       "2         22    6  55  0\n",
       "3         89  111  12  3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Clusters</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n    <tr>\n      <th>Labels</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>35</td>\n      <td>13</td>\n      <td>58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>22</td>\n      <td>6</td>\n      <td>55</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>89</td>\n      <td>111</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 345
    }
   ],
   "source": [
    "df = pd.DataFrame({'Labels': y, 'Clusters':kmeans_cluster.labels_})\n",
    "ct = pd.crosstab(df['Labels'],df['Clusters'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.41      0.55      0.47       106\n           1       0.00      0.00      0.00        26\n           2       0.00      0.00      0.00        83\n           3       0.70      0.94      0.81       215\n\n    accuracy                           0.61       430\n   macro avg       0.28      0.37      0.32       430\nweighted avg       0.45      0.61      0.52       430\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y,new_labels))"
   ]
  },
  {
   "source": [
    "Maybe trying different scaling or not scaling at all would help"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax = pd.DataFrame(MinMaxScaler().fit_transform(X_npf), columns=X_npf.columns)\n",
    "X_standard = pd.DataFrame(StandardScaler().fit_transform(X_npf), columns=X_npf.columns)\n",
    "X_noscaling = X_npf"
   ]
  },
  {
   "source": [
    "Feature selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_features = best_feature_columns(X_minmax,y,20)\n",
    "standard_features = best_feature_columns(X_standard,y,20)\n",
    "noscaling_features = best_feature_columns(X_noscaling,y,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for MinMax scaling 0.5651162790697675\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 3, 3]"
      ]
     },
     "metadata": {},
     "execution_count": 357
    }
   ],
   "source": [
    "# Minmax scaling result\n",
    "\n",
    "cluster = KMeans(4,n_init=50,random_state=42)\n",
    "minmax_cluster = cluster.fit(X_minmax[minmax_features])\n",
    "permutation = find_permutation(4,y,minmax_cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in minmax_cluster.labels_] \n",
    "print(\"Accuracy score is for MinMax scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for Standard scaling 0.586046511627907\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2, 3, 0, 3]"
      ]
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "# Standard scaling result\n",
    "\n",
    "cluster = KMeans(4,n_init=50,random_state=42)\n",
    "stamdard_cluster = cluster.fit(X_standard[standard_features])\n",
    "permutation = find_permutation(4,y,stamdard_cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in stamdard_cluster.labels_] \n",
    "print(\"Accuracy score is for Standard scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for no scaling 0.5790697674418605\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2, 3, 0, 3]"
      ]
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "# No scaling result\n",
    "\n",
    "cluster = KMeans(4,n_init=50,random_state=42)\n",
    "nofeatures_cluster = cluster.fit(X_noscaling[noscaling_features])\n",
    "permutation = find_permutation(4,y,nofeatures_cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in nofeatures_cluster.labels_] \n",
    "print(\"Accuracy score is for no scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "source": [
    "Well this is kind of odd? Class Ia is totally missing from the results? Let's try hieragical clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for no scaling 0.5720930232558139\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 2, 3, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 306
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Zero mean and unit variance scaling\n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean',linkage='ward')\n",
    "data = X[features].values\n",
    "cluster.fit(data)\n",
    "permutation = find_permutation(4,y,cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in cluster.labels_] \n",
    "print(\"Accuracy score is for no scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for no scaling 0.586046511627907\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 0, 0, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 307
    }
   ],
   "source": [
    "# Minmax scaling\n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean',linkage='ward')\n",
    "data = X_minmax[minmax_features].values\n",
    "cluster.fit(data)\n",
    "permutation = find_permutation(4,y,cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in cluster.labels_] \n",
    "print(\"Accuracy score is for no scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for no scaling 0.5720930232558139\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 2, 3, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "# Standard scaling\n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean',linkage='ward')\n",
    "data = X_standard[standard_features].values\n",
    "cluster.fit(data)\n",
    "permutation = find_permutation(4,y,cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in cluster.labels_] \n",
    "print(\"Accuracy score is for no scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score is for no scaling 0.5767441860465117\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 2, 3, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 309
    }
   ],
   "source": [
    "# No scaling\n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean',linkage='ward')\n",
    "data = X_noscaling[noscaling_features].values\n",
    "cluster.fit(data)\n",
    "permutation = find_permutation(4,y,cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in cluster.labels_] \n",
    "print(\"Accuracy score is for no scaling\", accuracy_score(y, new_labels))\n",
    "permutation"
   ]
  },
  {
   "source": [
    "Maybe it is just that it can't be predicted so well. I'd go with the k-means clustering. \n",
    "\n",
    "Last, let's try different amount of features. I will use StandardScaler as it got the best accuracy with most classes present"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best accuracy score is for 18 features with acc of: 0.586046511627907 the permutation being [2, 3, 0, 3]\n['Glob.mean', 'NET.mean', 'PAR.mean', 'RGlob.mean', 'RHIRGA168.mean', 'RHIRGA336.mean', 'RHIRGA42.mean', 'RHIRGA504.mean', 'RHIRGA672.mean', 'RHIRGA84.mean', 'UV_A.mean', 'UV_B.mean']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.586046511627907"
      ]
     },
     "metadata": {},
     "execution_count": 404
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "best_score = 0\n",
    "best_permu = []\n",
    "n_features = 0\n",
    "best_features = []\n",
    "for i in range(1,len(X_noscaling.columns)):\n",
    "    standard_features = best_feature_columns(X_standard,y,12)\n",
    "    cluster = KMeans(4,n_init=50,random_state=i)\n",
    "    standard_cluster = cluster.fit(X_standard[standard_features])\n",
    "    permutation = find_permutation(4,y,standard_cluster.labels_)\n",
    "    new_labels = [ permutation[label] for label in stamdard_cluster.labels_] \n",
    "    acc = accuracy_score(y, new_labels)\n",
    "\n",
    "    if (acc > best_score):\n",
    "        best_score = acc\n",
    "        best_permu = permutation\n",
    "        n_features = i\n",
    "        best_features = standard_features\n",
    "\n",
    "print(\"Best accuracy score is for {} features with acc of: {} the permutation being {}\".format(n_features, best_score,best_permu))\n",
    "print(best_features)\n",
    "0.586046511627907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ccuracy score is for 12 features with acc of: 0.586046511627907 the permutation being [2, 3, 0, 3]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Glob.mean',\n",
       " 'NET.mean',\n",
       " 'PAR.mean',\n",
       " 'RGlob.mean',\n",
       " 'RHIRGA168.mean',\n",
       " 'RHIRGA336.mean',\n",
       " 'RHIRGA42.mean',\n",
       " 'RHIRGA504.mean',\n",
       " 'RHIRGA672.mean',\n",
       " 'RHIRGA84.mean',\n",
       " 'UV_A.mean',\n",
       " 'UV_B.mean']"
      ]
     },
     "metadata": {},
     "execution_count": 405
    }
   ],
   "source": [
    "n_features = 12\n",
    "standard_features = best_feature_columns(X_standard,y,n_features)\n",
    "cluster = KMeans(4,n_init=50,random_state=18)\n",
    "standard_cluster = cluster.fit(X_standard[standard_features])\n",
    "permutation = find_permutation(4,y,standard_cluster.labels_)\n",
    "new_labels = [ permutation[label] for label in stamdard_cluster.labels_] \n",
    "acc = accuracy_score(y, new_labels)\n",
    "print(\"ccuracy score is for {} features with acc of: {} the permutation being {}\".format(n_features, acc,permutation))\n",
    "standard_features"
   ]
  },
  {
   "source": [
    "Trying some supervised learning now\n",
    " -Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.71\nAccuracy of Logistic regression classifier on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "# X being X_standard which is just standard scaled X values\n",
    "y = npf['class4']\n",
    "\n",
    "# Splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "source": [
    "Trying the same with best 12 features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Glob.mean',\n",
    " 'NET.mean',\n",
    " 'PAR.mean',\n",
    " 'RGlob.mean',\n",
    " 'RHIRGA168.mean',\n",
    " 'RHIRGA336.mean',\n",
    " 'RHIRGA42.mean',\n",
    " 'RHIRGA504.mean',\n",
    " 'RHIRGA672.mean',\n",
    " 'RHIRGA84.mean',\n",
    " 'UV_A.mean',\n",
    " 'UV_B.mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.62\nAccuracy of Logistic regression classifier on test set: 0.66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard[features], y, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "source": [
    "Oh, worse score. Maybe well just include all features. Let's try Decission Tree next"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\nAccuracy of Decision Tree classifier on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.75\nAccuracy of K-NN classifier on test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "# K neighbour classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of LDA classifier on training set: 0.76\nAccuracy of LDA classifier on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of GNB classifier on training set: 0.56\nAccuracy of GNB classifier on test set: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of SVM classifier on training set: 0.73\nAccuracy of SVM classifier on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "source": [
    "# SVM and logistic regression gave the best accuracies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[17  0  1  5]\n [ 3  0  0  1]\n [13  3  5  4]\n [ 1  0  1 54]]\n              precision    recall  f1-score   support\n\n          II       0.50      0.74      0.60        23\n          Ia       0.00      0.00      0.00         4\n          Ib       0.71      0.20      0.31        25\n    nonevent       0.84      0.96      0.90        56\n\n    accuracy                           0.70       108\n   macro avg       0.51      0.48      0.45       108\nweighted avg       0.71      0.70      0.67       108\n\n"
     ]
    }
   ],
   "source": [
    "# This is for logreg\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[15  0  4  4]\n [ 2  0  0  2]\n [16  0  6  3]\n [ 1  0  0 55]]\n              precision    recall  f1-score   support\n\n          II       0.44      0.65      0.53        23\n          Ia       0.00      0.00      0.00         4\n          Ib       0.60      0.24      0.34        25\n    nonevent       0.86      0.98      0.92        56\n\n    accuracy                           0.70       108\n   macro avg       0.48      0.47      0.45       108\nweighted avg       0.68      0.70      0.67       108\n\n"
     ]
    }
   ],
   "source": [
    "# This is for SVM\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}